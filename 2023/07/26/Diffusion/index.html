<!DOCTYPE html>
<html>
<head>
    <meta http-equiv="content-type" content="text/html; charset=utf-8">
    <meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport">
    <title>
        Diffusion for 3D Generation - RitzzzZ 
    </title>
     
    
<link rel="stylesheet" href="/css/grid.css">

    
<link rel="stylesheet" href="/css/custom.css">

    
<link rel="stylesheet" href="/css/ringo.css">

     
        <link rel="icon" type="image/x-icon" href="/img/icon.png " />
     
     
     
        <meta name=" " content="" />
     
        <meta name=" " content="" />
     
 
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head>

  <body>
    <header id="header" class="clearfix" onclick="window.open('/', '_self')">
    <div class="site-name">
        <a href="/" id="logo" class="site-title">
            RitzzzZ
        </a>
        <p class="description site-description">
            <span style="padding-top:20px; font-size: 10px">
                An infinite journey
            </span>
        </p>
    </div>
</header>
<div id="sidebar" role="complementary">
    <section class="widget">
        <ul class="menu widget-list">
            
                <li class="menu-item">
                    <a href="/" class="menu-item-link">
                        Home
                    </a>
                </li>
                
                <li class="menu-item">
                    <a href="/about" class="menu-item-link">
                        About
                    </a>
                </li>
                
                <li class="menu-item">
                    <a href="/archives" class="menu-item-link">
                        Archives
                    </a>
                </li>
                
        </ul>
    </section>
    <section class="widget sidebar-foot">
        <ul class="widget-list">
            <li>Theme <a rel="nofollow" target="_blank" href="https://github.com/HeliumOI/hexo-theme-ringo">Ringo</a>
                by <a target="_blank" href="/ "> RitzzzZ  </a></li>
            <li>Proudly powered by  <a rel="nofollow" target="_blank" href="https://hexo.io/">Hexo</a></li>
        </ul>
    </section>
</div>

<div id="helpbar">
    <div class="back-to-top">
        <button id="back2top">↑</button>
        <script>
            back2top.onclick = function() {
                var movement = document.body.scrollTop || document.documentElement.scrollTop;
                scrollBy(0, -movement);
            }
        </script>
    </div>
</div>
      <main class="main">
        <div id="body">
          <div class="container">
            <div class="col-12" id="main" role="main">
    <article class="post post-atpost" itemscope itemtype="http://schema.org/BlogPosting">
        <div class="post-title">
            <h1 class="post-title post-title-atpage" itemprop="name headline">
                <a itemprop="url" href="/2023/07/26/Diffusion/">
                    Diffusion for 3D Generation
                </a>
            </h1>
        </div>
        <ul class="post-meta post-meta-atpage">
            <li class="post-time">
                2023-07-26
            </li>
            <li>
                <div class="article-category">
                    
                </div>
            </li>
        </ul>
        <div class="post-content" itemprop="articleBody">
            <h2 id="Geometry-Free"><a href="#Geometry-Free" class="headerlink" title="Geometry-Free"></a>Geometry-Free</h2><ul>
<li><p>NOVEL VIEW SYNTHESIS WITH DIFFUSION MODELS</p>
<p>  本文提出了3DiM，一个用于3D新视角合成的扩散模型。3DiM的核心部分是pose-conditional image-to-image diffution model，将source view和它的pose作为输入，生成目标pose的新视角。3DiM用一种新技术，stochastic conditioning，生成3D consistent的多视角。输出视角是<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/zh-cn/%E8%87%AA%E6%88%91%E8%BF%B4%E6%AD%B8%E6%A8%A1%E5%9E%8B">自回归</a>生成的，在每个新视角的生成过程中，在每一个denoising step中选一个随机的conditioning view。</p>
<p>  <img src="image.png" alt="Alt text"></p>
<p>贡献：</p>
<ol>
<li>3DiM, geometry-free image-to-image diffusion model，用于新视角合成</li>
<li>stochastic conditioning 采样算法，更3D-consistent的输出<br> <img src="image2.png" alt="Alt text"></li>
<li>新的UNet结构变体 X-UNet，在两个输入帧（clean conditioning view, denoising target view）之间共享同一个UNet权重，加入了cross attention layers融合输入和输出视角的信息</li>
<li>对geometry-free视角合成模型的新的评估方法：3D consistency scoring，通过在模型输出上训练神经场来数值地衡量3D consistency</li>
</ol>
<p>实验：SRN ShapeNet dataset，单视角生成higher fidelity, approximately 3D consistent</p>
</li>
</ul>
<h2 id="Geometry-Aware"><a href="#Geometry-Aware" class="headerlink" title="Geometry-Aware"></a>Geometry-Aware</h2><h3 id="in-2D-image-space"><a href="#in-2D-image-space" class="headerlink" title="in 2D image space"></a>in 2D image space</h3><p><strong>used in optimization</strong></p>
<p><em>with pretrained models</em></p>
<ul>
<li><p>DREAMFUSION: TEXT-TO-3D USING 2D DIFFUSION</p>
<p>  将扩散模型用于3D合成，需要大规模的带标签的3D数据集和高效的3D数据除噪结构，而目前我们不具备这些条件。此项工作通过使用预训练的2D text-to-image扩散模型进行text-to-3D合成，避免了这些限制。本文引入了基于概率密度蒸馏（probability density distillation）的loss，使2D扩散模型可以作为参数图像生成器优化的先验。将此loss用于类似DeepDream的过程，就可以通过梯度下降优化一个随机初始化的3D模型（NeRF），从任意视角渲染得到的2D图像都能达到较低的loss。文本对于的3D模型最终可以从任意视角被观察，重新打光，或加入任何3D环境。该方法不需要3D训练数据，也不需要修改图像扩散模型。</p>
<p>  <img src="image3.png" alt="Alt text"></p>
<p>  贡献：</p>
<ol>
<li>提出DreamFusion，一种新的类似NeRF的渲染引擎，但是DreamFusion不需要3D或者多视角的训练数据，只需要预训练的2D扩散模型</li>
<li>为了将2D图像扩散模型迁移到3D空间，本文提出一种新的方法，Score Distillation Sampling（SDS）</li>
</ol>
</li>
<li><p>Score Jacobian Chaining: Lifting Pretrained 2D Diffusion Models for 3D Generation</p>
<p>  扩散模型学习如何去预测梯度的向量场。本文提出对梯度使用链式法则，通过可微渲染器（被初始化为voxel radiance field）的Jacobian反向传播扩散模型的score。这种方法将多视角的2D scores融合进3D score，重新提出了用于3D生成的预训练2D模型。我们指出分布不匹配的技术难点，并提出了一种新的估计方法来解决它。我们在多个现成的扩散图像生成模型上跑了我们的算法，包括在LAION 5B数据集上训练的Stable Diffusion。</p>
<p>  贡献：</p>
<ol>
<li>提出了一种通过链式法则将2D扩散模型提升到3D的方法</li>
<li>阐释了使用预训练降噪器会遇到的out-of-distribution (OOD)问题，并提出Perturb-and-Average Scoring来解决该问题</li>
<li>指出了使用Perturb-and-Average Scoring作为梯度来进行优化的一些开放问题</li>
<li>阐述了Score Jacobian Chaining在3D文本驱动生成问题上的有效性</li>
</ol>
</li>
<li><p>NeRDi: Single-View NeRF Synthesis with Language-Guided Diffusion as General Image Priors</p>
<p>  2D-to-3D重建是一个ill-posed problem，人们用3D世界的先验知识来解决这个问题。本文提出NeRDi，一个带有从2D扩散模型获得的图像先验的单视角NeRF合成框架。将单视角重建看做 image-conditioned 3D生成问题，利用输入视角约束下的预训练扩散模型，通过最小化任意视角渲染的diffusion loss来优化NeRF representations。我们用现有的视觉-语言模型，并引入two-section语言引导作为扩散模型的conditioning input，这对提高多视角内容的一致性有很大帮助，因为它缩小了基于单视角图片的语义和视觉特征的图像先验的范围。另外，我们引入了一个基于估计的深度图的geometric loss，用来正则化NeRF的3D geometry。在DTU MVS数据集上的实验结果表明，我们的模型可以进行高质量的新视角合成。我们还说明了该模型在zero-shot（零样本）NeRF合成上的通用性，用于野外的图像合成。</p>
<p>  <img src="image4.png" alt="Alt text"></p>
<p>  loss = 固定的输入视角的重建损失 + 任意采样视角diffusion loss + 输入视角的depth correlation loss</p>
<p>  贡献：</p>
<ol>
<li>将单视角重建看成是conditioned 3D生成问题，并提出了单张图片的NeRF合成框架，不需要3D监督，而是使用在大规模图片数据集上训练的扩散模型的2D先验</li>
<li>设计了一个two-section semantic guidance，将在所有图像上通用的先验narrow down到和输入图像$x_0$相关的；two-section分别指image captioning/classification network和textual inversion，将它们各自产生的text embedding连接起来，作为guidance</li>
<li>在包含3D不确定性的估计的深度图中，引入geometric regularization term</li>
<li><p>在DTU MVS上验证了zero-shot新视角合成的结果，在in-the-wild images的新视角合成中能生成高质量图像</p>
<p>NeRDi vs. DreamFusion：都是利用2D image diffusions优化新视角的NeRF渲染结果；不同点是，DreamFusion是用用户指定的语言输入来进行无约束的NeRF生成，而NeRDi用单视角图像输入的特征来约束新视角的image distributions</p>
</li>
</ol>
</li>
<li><p>NeuralLift-360: Lifting An In-the-wild 2D Photo to A 3D Object with 360{\deg} Views </p>
<p>  本文提出NeuralLift-360，使用depth-aware NeRF并学习用扩散模型描述场景，可以用一张图片生成3D物体。通过引入ranking loss，NeuralLift-360可以被野外粗糙的深度估计引导。我们同时也对扩散先验采用了CLIP-guided采样策略，从而提升一致的guidance。</p>
<p>  <img src="image5.png" alt="Alt text"></p>
<p>  贡献：</p>
<ol>
<li>野外单张图片可以被lift到3D</li>
<li>提出CLIP-guided采样策略</li>
<li>当reference image很难被描述时，我们finetune扩散模型，同时保持生成多样内容的能力</li>
<li>介绍了一种利用ranking信息的scale-invariant depth supervision，从而减轻了对精确的多视角一致的深度估计的需求，拓展了本文算法的应用场景</li>
</ol>
</li>
<li><p>SceneScape: Text-Driven Consistent Scene Generation</p>
<p>  提出一种text-driven perpetual view generation的方法，对于一段描述场景和相机位姿的输入文字prompt，能够合成一段视频。我们的新模型将预训练的text-to-image模型的生成能力和预训练的单目深度预测模型的geometric priors结合起来，可以在线生成这样的视频。为了解决3D一致性的问题，我们部署了在线的测试时训练，鼓励预测得到的当前帧深度图和合成场景geometrically consistent。深度图被用来在视频生成过程中逐渐建立一个场景的unified mesh representation。</p>
<p>  <img src="image6.png" alt="Alt text"></p>
<p>  贡献：</p>
<ol>
<li>第一个text-driven perpetual view generation方法</li>
<li>第一个zero-shot/test-time场景生成方法，无需在特性的目标domain中进行大规模训练，就能合成多样的场景</li>
<li>通过逐步估计unified 3D representation，实现3D一致性生成</li>
</ol>
</li>
</ul>
<p><em>without pretrained models</em></p>
<ul>
<li><p>SparseFusion: Distilling View-conditioned Diffusion for 3D Reconstruction</p>
<p>  本文提出SparseFusion，一种稀疏视角3D重建方法，将神经渲染和概率图像生成的优势结合起来。目前的方法一般使用重新投影特征进行神经渲染，但是无法生成看不见的区域，也不能处理视角变化很大时的不确定性。其他方法将此看做一个概率2D合成任务，虽然能生成很好的2D图像，但是很难在3D空间中推理出一致性。我们发现这种3D一致性和概率生成之间的矛盾是没有必要的。实际上，我们展示了geometric consistency和生成推理可以以mode-seeking的形式互为补充。通过从view-conditioned latent diffusion model中蒸馏3D一致的场景表示，我们可以恢复出很好的3D表示，能够渲染出精准的、有真实感的图像。</p>
</li>
</ul>
<ul>
<li>NerfDiff: Single-image View Synthesis with NeRF-guided Distillation from 3D-aware Diffusion</li>
</ul>
<p><strong>used in generation</strong></p>
<h3 id="in-3D-Latent-space"><a href="#in-3D-Latent-space" class="headerlink" title="in 3D/Latent space"></a>in 3D/Latent space</h3><h2 id="Appendix"><a href="#Appendix" class="headerlink" title="Appendix"></a>Appendix</h2><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/612572004">Stable Diffusion以及几种scheduler的对比</a></p>
<p>Ancestral sampling</p>

             
        </div>
        
            <p itemprop="keywords" class="tags">
                
                    <a href="/tags/ML/"> ML </a>
                
            </p>
        
    </article>
    <div class="post-near">
    <div class="post-near-child post-near-child-left "> 
        
            <a href="/2023/07/22/SSDNeRF-ICCV-2023/">SSDNeRF (ICCV 2023) &laquo; </a>
        
        <br /> Prev  &laquo;
    </div>
    <div class="post-near-child post-near-child-right">
        
            <a href="/2023/07/30/Probability/"> &raquo; Probability</a>
        
        <br /> &raquo; Next 
    </div>
</div>
</div>
             
<div id="comments">
     
</div>
 
            <footer id="footer" role="contentinfo">
    
        &copy; 2022 - 2023
        <br />
    
    
    <br />
    
    
        <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
        <span id="busuanzi_value_site_pv">......</span> visits ·
        <span id="busuanzi_value_site_uv">......</span> visitors 
    
</footer>
          </div>
        </div>
      </main>
      <!-- highlight support -->

    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.1.0/build/styles/default.min.css">


<script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.1.0/build/highlight.min.js"></script>
 
<script>
        hljs.initHighlightingOnLoad();
</script>
 
<!-- prettify support -->

    
<link rel="stylesheet" href="/prettify/prettify.css">


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/color-themes-for-google-code-prettify@2.0.4/dist/themes/tomorrow.min.css">


<script src="/prettify/prettify.js"></script>


<script>
    let prettifyel = document.querySelectorAll('pre');
    for (let i = 0; i < (prettifyel || []).length; i += 1) {
        prettifyel[i].classList.add('prettyprint');
        prettifyel[i].classList.add('linenums');
    }
    PR.prettyPrint();
</script>
 
<!-- mathjax support -->

    
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"] ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>


<!-- fancybox support -->
 
<!-- viewerjs support -->

    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/viewerjs@1.10.0/dist/viewer.min.css">


<script src="https://cdn.jsdelivr.net/npm/viewerjs@1.10.0/dist/viewer.min.js"></script>

<script type="text/javascript">
    Viewer.setDefaults({
        zoomRatio: [0.5],
        show: function () {
            this.viewer.zoomTo(1);
        },
    });
    
    var imageList = document.querySelector('.post-content').getElementsByTagName('img');
    
    var imageArray = new Array();
    Array.prototype.forEach.call(imageList, element => {
        if (element.alt != "no-view" && element.className != "no-view") {
            imageArray.push(element);
        }
    });
    
    Array.prototype.forEach.call(imageArray, element => {
        var viewer1 = new Viewer(element);
        viewer1.images = imageArray;
        viewer1.length = imageArray.length;
    });
</script>
 
<!-- google analytics support -->



 
 

<!-- lazyload support -->

    
<script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.4.0/dist/lazyload.min.js"></script>

<script>
    new LazyLoad({
        elements_selector: '.post-content img'
    });
</script>
 
  </body>

</html>